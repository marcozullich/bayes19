<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Bayesian modelling | Exam project</title>
  <meta name="description" content="4 Bayesian modelling | Exam project" />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Bayesian modelling | Exam project" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Bayesian modelling | Exam project" />
  
  
  

<meta name="author" content="Marco Zullich" />


<meta name="date" content="2019-06-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="preliminary-analysis.html">
<link rel="next" href="conclusion.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>1</b> Prerequisites</a></li>
<li class="chapter" data-level="2" data-path="problem-specification.html"><a href="problem-specification.html"><i class="fa fa-check"></i><b>2</b> Problem specification</a></li>
<li class="chapter" data-level="3" data-path="preliminary-analysis.html"><a href="preliminary-analysis.html"><i class="fa fa-check"></i><b>3</b> Preliminary analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="preliminary-analysis.html"><a href="preliminary-analysis.html#univariate-analysis"><i class="fa fa-check"></i><b>3.1</b> Univariate analysis</a><ul>
<li class="chapter" data-level="3.1.1" data-path="preliminary-analysis.html"><a href="preliminary-analysis.html#numerical-variables"><i class="fa fa-check"></i><b>3.1.1</b> Numerical variables</a></li>
<li class="chapter" data-level="3.1.2" data-path="preliminary-analysis.html"><a href="preliminary-analysis.html#categorical-variables"><i class="fa fa-check"></i><b>3.1.2</b> Categorical variables</a></li>
<li class="chapter" data-level="3.1.3" data-path="preliminary-analysis.html"><a href="preliminary-analysis.html#missing-values"><i class="fa fa-check"></i><b>3.1.3</b> Missing values</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="preliminary-analysis.html"><a href="preliminary-analysis.html#bivariate-analysis"><i class="fa fa-check"></i><b>3.2</b> Bivariate analysis</a><ul>
<li class="chapter" data-level="3.2.1" data-path="preliminary-analysis.html"><a href="preliminary-analysis.html#firstbiv"><i class="fa fa-check"></i><b>3.2.1</b> <code>attain</code> and <code>verbal</code></a></li>
<li class="chapter" data-level="3.2.2" data-path="preliminary-analysis.html"><a href="preliminary-analysis.html#attain-and-sex"><i class="fa fa-check"></i><b>3.2.2</b> <code>attain</code> and <code>sex</code></a></li>
<li class="chapter" data-level="3.2.3" data-path="preliminary-analysis.html"><a href="preliminary-analysis.html#attain-and-social-category"><i class="fa fa-check"></i><b>3.2.3</b> <code>attain</code> and social category</a></li>
<li class="chapter" data-level="3.2.4" data-path="preliminary-analysis.html"><a href="preliminary-analysis.html#attain-and-secondary-school"><i class="fa fa-check"></i><b>3.2.4</b> <code>attain</code> and secondary school</a></li>
<li class="chapter" data-level="3.2.5" data-path="preliminary-analysis.html"><a href="preliminary-analysis.html#social-composition-for-primary-schools"><i class="fa fa-check"></i><b>3.2.5</b> social composition for primary schools</a></li>
<li class="chapter" data-level="3.2.6" data-path="preliminary-analysis.html"><a href="preliminary-analysis.html#attain-and-primary-school"><i class="fa fa-check"></i><b>3.2.6</b> attain and primary school</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bayesian-modelling.html"><a href="bayesian-modelling.html"><i class="fa fa-check"></i><b>4</b> Bayesian modelling</a><ul>
<li class="chapter" data-level="4.1" data-path="bayesian-modelling.html"><a href="bayesian-modelling.html#attain01-verbal"><i class="fa fa-check"></i><b>4.1</b> <code>attain01</code> ~ <code>verbal</code></a><ul>
<li class="chapter" data-level="4.1.1" data-path="bayesian-modelling.html"><a href="bayesian-modelling.html#coefficients-analyisis"><i class="fa fa-check"></i><b>4.1.1</b> Coefficients analyisis</a></li>
<li class="chapter" data-level="4.1.2" data-path="bayesian-modelling.html"><a href="bayesian-modelling.html#posterior-predictive-checks"><i class="fa fa-check"></i><b>4.1.2</b> Posterior predictive checks</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="bayesian-modelling.html"><a href="bayesian-modelling.html#hierarchical-models"><i class="fa fa-check"></i><b>4.2</b> Hierarchical models</a><ul>
<li class="chapter" data-level="4.2.1" data-path="bayesian-modelling.html"><a href="bayesian-modelling.html#hierarchy-over-social-category"><i class="fa fa-check"></i><b>4.2.1</b> Hierarchy over social category</a></li>
<li class="chapter" data-level="4.2.2" data-path="bayesian-modelling.html"><a href="bayesian-modelling.html#hierarchy-over-primary-school"><i class="fa fa-check"></i><b>4.2.2</b> Hierarchy over primary school</a></li>
<li class="chapter" data-level="4.2.3" data-path="bayesian-modelling.html"><a href="bayesian-modelling.html#hierarchy-over-secondary-school"><i class="fa fa-check"></i><b>4.2.3</b> Hierarchy over secondary school</a></li>
<li class="chapter" data-level="4.2.4" data-path="bayesian-modelling.html"><a href="bayesian-modelling.html#cross-hierarchy-with-secondary-school-and-social-extraction"><i class="fa fa-check"></i><b>4.2.4</b> Cross-hierarchy with secondary school and social extraction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>5</b> Conclusion</a><ul>
<li class="chapter" data-level="5.1" data-path="conclusion.html"><a href="conclusion.html#possible-improvements"><i class="fa fa-check"></i><b>5.1</b> Possible improvements</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="appendix-stan-files.html"><a href="appendix-stan-files.html"><i class="fa fa-check"></i><b>6</b> Appendix - Stan files</a><ul>
<li class="chapter" data-level="6.1" data-path="appendix-stan-files.html"><a href="appendix-stan-files.html#simple-logistic-regression-with-one-predictor"><i class="fa fa-check"></i><b>6.1</b> Simple logistic regression with one predictor</a></li>
<li class="chapter" data-level="6.2" data-path="appendix-stan-files.html"><a href="appendix-stan-files.html#hierarchical-logistic-regression-with-one-predictor-and-group-variable-intercept"><i class="fa fa-check"></i><b>6.2</b> Hierarchical logistic regression with one predictor and group, variable intercept</a><ul>
<li class="chapter" data-level="6.2.1" data-path="appendix-stan-files.html"><a href="appendix-stan-files.html#more-informative-priors"><i class="fa fa-check"></i><b>6.2.1</b> More informative priors</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="appendix-stan-files.html"><a href="appendix-stan-files.html#hierarchical-logistic-regression-with-one-predictor-and-group-variable-intercepts-and-slopes"><i class="fa fa-check"></i><b>6.3</b> Hierarchical logistic regression with one predictor and group, variable intercepts and slopes</a></li>
<li class="chapter" data-level="6.4" data-path="appendix-stan-files.html"><a href="appendix-stan-files.html#simple-probit-regression-with-one-predictor"><i class="fa fa-check"></i><b>6.4</b> Simple probit regression with one predictor</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Exam project</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian-modelling" class="section level1">
<h1><span class="header-section-number">4</span> Bayesian modelling</h1>
<div id="attain01-verbal" class="section level2">
<h2><span class="header-section-number">4.1</span> <code>attain01</code> ~ <code>verbal</code></h2>
<p>As first mentioned before, the first model I’ll try to fit will be <code>attain01</code> ~ <code>verbal</code>.</p>
<p>I have already proven, in a frequentist framework, in <a href="preliminary-analysis.html#firstbiv">3.2.1</a>, that there exists a correlation between the two variables. This will be tested in a bayesian framework.</p>
<p><code>attain01</code> is a dichotomic variable, hence the model that I’ll fit is a logistic regression (with canonical link). I’ll also pick a vague prior on the coefficients: being the sample size large, in all chances the likelihood will largely dominate on the prior.</p>
<p>This is the model specification:</p>
<p><span class="math display">\[
\text{attain01}_i \sim Bernoulli(\text{logit}^{-1}(\eta_i)) \\
\eta_i = \alpha + \beta \cdot \text{verbal}_i \\
\alpha \sim N(0,10) \\
\beta \sim N(0,2.5)
\]</span></p>
<p>Note that the priors for <span class="math inline">\(\alpha, \beta\)</span> have been chosen as the default for <code>stan_glm</code> of <code>rstanarm</code> package.</p>
<p>Model compile:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_lin_<span class="dv">1</span> =<span class="st"> </span><span class="kw">stan_model</span>(<span class="st">&#39;models/stan_model_glm_1.stan&#39;</span>)</code></pre></div>
<p>and fit:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_lin_<span class="dv">1</span> =<span class="st"> </span><span class="kw">sampling</span>(model_lin_<span class="dv">1</span>,
                     <span class="dt">data =</span> <span class="kw">list</span>(<span class="dt">N =</span> data <span class="op">%&gt;%</span><span class="st"> </span>nrow,
                                 <span class="dt">x =</span> data<span class="op">$</span>verbal,
                                 <span class="dt">y =</span> data<span class="op">$</span>attain01 <span class="op">%&gt;%</span>
<span class="st">                                   </span>as.character <span class="op">%&gt;%</span>
<span class="st">                                   </span>as.numeric),
                     <span class="dt">cores =</span> <span class="dv">4</span>)

<span class="kw">saveRDS</span>(fit_lin_<span class="dv">1</span>, <span class="st">&quot;models/fit_lin_1.rds&quot;</span>)</code></pre></div>
<div id="coefficients-analyisis" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Coefficients analyisis</h3>
<p>These are the coefficients estimated by the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(fit_lin_<span class="dv">1</span>, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>,<span class="st">&quot;beta&quot;</span>))</code></pre></div>
<pre><code>## Inference for Stan model: stan_model_glm_1.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##       mean se_mean   sd 2.5%  25%  50%  75% 97.5% n_eff Rhat
## alpha 0.21       0 0.05 0.12 0.18 0.21 0.24  0.30  2681    1
## beta  0.16       0 0.01 0.15 0.16 0.16 0.17  0.17  3549    1
## 
## Samples were drawn using NUTS(diag_e) at Thu Jun 20 17:22:10 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>As expected, the coefficients have the same mean as the frequentist glm’s estimate (see <a href="preliminary-analysis.html#firstbiv">3.2.1</a>). Both have 0 well outside their 95% C.I.</p>
<p>The interpretation of <span class="math inline">\(\beta\)</span> does not change: it has a positive effect on determining the sufficiency in the test; for a unitary increase in <code>verbal</code>, there’s an increase of .16 in the linear predictor, or in the log-odds of the probability of success.</p>
</div>
<div id="posterior-predictive-checks" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Posterior predictive checks</h3>
<p>First, I’m going to have a look at the fit of the mass function of the replicated data to the original y:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y =<span class="st"> </span>data<span class="op">$</span>attain01 <span class="op">%&gt;%</span><span class="st"> </span>as.character <span class="op">%&gt;%</span><span class="st"> </span>as.numeric
y_rep =<span class="st"> </span><span class="kw">as.matrix</span>(fit_lin_<span class="dv">1</span>, <span class="dt">pars =</span> <span class="st">&quot;y_rep&quot;</span>)

<span class="kw">ppc_ecdf_overlay</span>(y,y_rep,<span class="dt">discrete =</span> T)</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>A check can be done on the bernoulli’s parameter <span class="math inline">\(\theta\)</span>, which in our model is, on average <span class="math inline">\(\text{logit}^{-1}(\alpha+x*\beta)\)</span>.</p>
<p>Being our observed and replicated data just vectors of 0s and 1s, the parameter in an observed or replicated array is the mean of the values of such array.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ppc_stat</span>(y,y_rep)</code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>The fit is excellent also considering the parameter.</p>
<p>To have an estimate for residuals, I’ll fit the Brier Score to the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hist_fun =<span class="st"> </span><span class="cf">function</span>(y, y_rep, fun){
  
  
  vals =<span class="st"> </span>y_rep <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">apply</span>(<span class="dv">1</span>,<span class="cf">function</span>(x) <span class="kw">fun</span>(y, x) )
  
  <span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;average statistic = &quot;</span>,<span class="kw">mean</span>(vals)))
  <span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;sd = &quot;</span>,<span class="kw">sd</span>(vals)))
  
  <span class="kw">return</span> (<span class="kw">ggplot</span>(<span class="dt">data =</span> vals <span class="op">%&gt;%</span><span class="st"> </span>as_tibble, <span class="kw">aes</span>(<span class="dt">x=</span>value)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y=</span>..density..),  <span class="dt">fill =</span> <span class="st">&quot;orange&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_density</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">xlab</span> ( <span class="kw">as.character</span>(<span class="kw">substitute</span>(fun)) ) )
}

(<span class="dt">brier_1 =</span> <span class="kw">hist_fun</span>(y, y_rep, DescTools<span class="op">::</span>BrierScore))</code></pre></div>
<pre><code>## [1] &quot;average statistic = 0.28458114992722&quot;
## [1] &quot;sd = 0.00726369957376172&quot;</code></pre>
<pre><code>## Warning: Calling `as_tibble()` on a vector is discouraged, because the behavior is likely to change in the future. Use `tibble::enframe(name = NULL)` instead.
## This warning is displayed once per session.</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>An average value of 0.28 indicates a moderate fit.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="dt">loo_1 =</span> loo<span class="op">::</span><span class="kw">loo</span>(loo<span class="op">::</span><span class="kw">extract_log_lik</span>(fit_lin_<span class="dv">1</span>)))</code></pre></div>
<pre><code>## Warning: Relative effective sample sizes (&#39;r_eff&#39; argument) not specified.
## For models fit with MCMC, the reported PSIS effective sample sizes and 
## MCSE estimates will be over-optimistic.</code></pre>
<pre><code>## 
## Computed from 4000 by 3435 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1504.7 31.8
## p_loo         2.0  0.1
## looic      3009.4 63.6
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &lt; 0.5).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
</div>
</div>
<div id="hierarchical-models" class="section level2">
<h2><span class="header-section-number">4.2</span> Hierarchical models</h2>
<p>I provide two generic <code>stan</code> models for the following sections:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_lin_hier_interc =<span class="st"> </span><span class="kw">stan_model</span>(<span class="st">&#39;models/stan_model_glm_2.stan&#39;</span>, <span class="dt">auto_write =</span> T)

model_lin_hier_both =<span class="st"> </span><span class="kw">stan_model</span>(<span class="st">&#39;models/stan_model_glm_3.stan&#39;</span>, <span class="dt">auto_write =</span> T)

model_lin_hier_interc_s =<span class="st"> </span><span class="kw">stan_model</span>(<span class="st">&quot;models/stan_model_glm_2_pri_strong.stan&quot;</span>)</code></pre></div>
<div id="hierarchy-over-social-category" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Hierarchy over social category</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_lin_hier_soc_interc =<span class="st"> </span><span class="kw">sampling</span>(model_lin_hier_interc, 
  <span class="dt">data =</span> <span class="kw">list</span>(<span class="dt">N =</span> data <span class="op">%&gt;%</span><span class="st"> </span>nrow,
               <span class="dt">x =</span> data<span class="op">$</span>verbal,
               <span class="dt">y =</span> y,
               <span class="dt">G =</span> data<span class="op">$</span>social_cat<span class="op">%&gt;%</span>levels<span class="op">%&gt;%</span>length,
               <span class="dt">group_mapping =</span> data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(social_cat) <span class="op">%&gt;%</span>
<span class="st">                                </span>as.numeric),
  <span class="dt">cores =</span> <span class="dv">4</span>)

<span class="kw">saveRDS</span>(fit_lin_hier_soc_interc, <span class="st">&quot;models/fit_lin_soc.rds&quot;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mcmc_intervals</span>(<span class="kw">as.matrix</span>(fit_lin_hier_soc_interc), <span class="dt">pars =</span> 
                 <span class="kw">dimnames</span>(fit_lin_hier_soc_interc)<span class="op">$</span>parameters <span class="op">%&gt;%</span>
<span class="st">                 </span>as_tibble <span class="op">%&gt;%</span>
<span class="st">                 </span><span class="kw">filter</span>(<span class="kw">grepl</span>(<span class="st">&quot;alpha|beta&quot;</span>,value)) <span class="op">%&gt;%</span><span class="st"> </span>pull)</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<p>As expected, there’re different values of <span class="math inline">\(\alpha\)</span> for medium and high class. <span class="math inline">\(\beta\)</span> is still significative with a very precise posterior.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ppc_stat_grouped</span>(y, <span class="dt">yrep =</span> <span class="kw">as.matrix</span>(fit_lin_hier_soc_interc,
                                    <span class="dt">pars =</span> <span class="st">&quot;y_rep&quot;</span>),
                 <span class="dt">group =</span> data<span class="op">$</span>social_cat)</code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist_fun</span>(y, <span class="kw">as.matrix</span>(fit_lin_hier_soc_interc,
                                    <span class="dt">pars =</span> <span class="st">&quot;y_rep&quot;</span>),
         DescTools<span class="op">::</span>BrierScore)</code></pre></div>
<pre><code>## [1] &quot;average statistic = 0.276551382823872&quot;
## [1] &quot;sd = 0.00702673041650985&quot;</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="dt">loo_soc=</span>loo<span class="op">::</span><span class="kw">loo</span>(loo<span class="op">::</span><span class="kw">extract_log_lik</span>(fit_lin_hier_soc_interc)))</code></pre></div>
<pre><code>## Warning: Relative effective sample sizes (&#39;r_eff&#39; argument) not specified.
## For models fit with MCMC, the reported PSIS effective sample sizes and 
## MCSE estimates will be over-optimistic.</code></pre>
<pre><code>## 
## Computed from 4000 by 3435 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1469.8 32.0
## p_loo         3.9  0.2
## looic      2939.6 63.9
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &lt; 0.5).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<div id="model-with-variable-intercept-and-slope" class="section level4">
<h4><span class="header-section-number">4.2.1.1</span> Model with variable intercept and slope</h4>
<p>To improve the result of the hierarchical model, I’ll be fitting a model with variable intercept and slope over the same hierachy.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_lin_hier_soc_both =<span class="st"> </span><span class="kw">sampling</span>(model_lin_hier_both, 
  <span class="dt">data =</span> <span class="kw">list</span>(<span class="dt">N =</span> data <span class="op">%&gt;%</span><span class="st"> </span>nrow,
               <span class="dt">x =</span> data<span class="op">$</span>verbal,
               <span class="dt">y =</span> y,
               <span class="dt">G =</span> data<span class="op">$</span>social_cat<span class="op">%&gt;%</span>levels<span class="op">%&gt;%</span>length,
               <span class="dt">group_mapping =</span> data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(social_cat) <span class="op">%&gt;%</span>
<span class="st">                                </span>as.numeric),
  <span class="dt">cores =</span> <span class="dv">4</span>)

<span class="kw">saveRDS</span>(fit_lin_hier_soc_both, <span class="st">&quot;models/fit_lin_4.rds&quot;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mcmc_intervals</span>(<span class="kw">as.matrix</span>(fit_lin_hier_soc_both), <span class="dt">pars =</span> 
                 <span class="kw">dimnames</span>(fit_lin_hier_soc_both)<span class="op">$</span>parameters <span class="op">%&gt;%</span>
<span class="st">                 </span>as_tibble <span class="op">%&gt;%</span>
<span class="st">                 </span><span class="kw">filter</span>(<span class="kw">grepl</span>(<span class="st">&quot;alpha|beta&quot;</span>,value)) <span class="op">%&gt;%</span><span class="st"> </span>pull)</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<p>The model doesn’t seem to add up to what we’ve seen before, because the <span class="math inline">\(\beta\)</span>s have all a very similar posterior distribution. I’ll have a look at the Brier score as a last check:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="dt">loo_soc_both=</span>loo<span class="op">::</span><span class="kw">loo</span>(loo<span class="op">::</span><span class="kw">extract_log_lik</span>(fit_lin_hier_soc_both)))</code></pre></div>
<pre><code>## Warning: Relative effective sample sizes (&#39;r_eff&#39; argument) not specified.
## For models fit with MCMC, the reported PSIS effective sample sizes and 
## MCSE estimates will be over-optimistic.</code></pre>
<pre><code>## 
## Computed from 4000 by 3435 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1471.8 32.1
## p_loo         6.2  0.4
## looic      2943.5 64.1
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &lt; 0.5).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
</div>
</div>
<div id="hierarchy-over-primary-school" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Hierarchy over primary school</h3>
<div id="model-with-variable-intercept-fixed-slope" class="section level4">
<h4><span class="header-section-number">4.2.2.1</span> Model with variable intercept, fixed slope</h4>
<p>Having seen that the previous hierarchy did not offer much contribution to the model, I decided to change the hierarchical structure, focusing on the primary school of origin.</p>
<p>This model will begin in trying to answer to the request nr. 3:</p>
<pre><code>Draw inference on school random effects. Does the primary school matter?</code></pre>
<p>The model will be a hierarchical bayesian logistic regression with a random effect for the primary school:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_lin_hier_pri_interc =<span class="st"> </span><span class="kw">sampling</span>(model_lin_hier_interc, 
  <span class="dt">data =</span> <span class="kw">list</span>(<span class="dt">N =</span> data <span class="op">%&gt;%</span><span class="st"> </span>nrow,
               <span class="dt">x =</span> data<span class="op">$</span>verbal,
               <span class="dt">y =</span> y,
               <span class="dt">G =</span> data<span class="op">$</span>primary<span class="op">%&gt;%</span>levels<span class="op">%&gt;%</span>length,
               <span class="dt">group_mapping =</span> data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(primary) <span class="op">%&gt;%</span>
<span class="st">                                </span>as.numeric),
  <span class="dt">cores =</span> <span class="dv">4</span>)

<span class="kw">saveRDS</span>(fit_lin_hier_pri_interc, <span class="st">&quot;models/fit_lin_2.rds&quot;</span>)</code></pre></div>
<p>Let’s analyze the posterior in an automated fashion:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_lin_sum =<span class="st"> </span><span class="kw">summary</span>(fit_lin_hier_pri_interc)
fit_lin_sum =<span class="st"> </span>fit_lin_sum<span class="op">$</span>summary <span class="op">%&gt;%</span>
<span class="st">  </span>as_tibble <span class="op">%&gt;%</span>
<span class="st">  </span>tibble<span class="op">::</span><span class="kw">add_column</span>(<span class="dt">param =</span> fit_lin_sum<span class="op">$</span>summary <span class="op">%&gt;%</span><span class="st"> </span>row.names) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(param, <span class="kw">everything</span>())</code></pre></div>
<ul>
<li><p><span class="math inline">\(\beta\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_lin_sum <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(param <span class="op">==</span><span class="st"> &#39;beta&#39;</span>)</code></pre></div>
<pre><code>## # A tibble: 1 x 11
##   param  mean  se_mean      sd `2.5%` `25%` `50%` `75%` `97.5%` n_eff  Rhat
##   &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 beta  0.183  8.75e-5 0.00634  0.171 0.179 0.183 0.187   0.195 5250. 0.999</code></pre>
<p>The mean is very close to the non-hierarchical model (<span class="math inline">\(\Delta \pm 0.02\)</span>), and 0 is well outside the 95% CI.</p></li>
<li><p>Intercepts</p>
<p>Instead of printing directly the intercepts’ posteriors, I’ll store them in a data frame and make considerations on the 95% CI via <code>dplyr</code>.</p>
<p>First, I create the dataframe and add a new variable telling me whether 0 is or is not within the 95% CI of each parameter:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_lin_sum_alphas =<span class="st"> </span>fit_lin_sum <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">grepl</span>(<span class="st">&#39;alpha&#39;</span>,param)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">sign =</span> <span class="kw">ifelse</span>(<span class="st">`</span><span class="dt">97.5%</span><span class="st">`</span><span class="op">*</span><span class="st">`</span><span class="dt">2.5%</span><span class="st">`</span><span class="op">&lt;</span><span class="dv">0</span>, <span class="st">&quot;NoSign&quot;</span>, <span class="st">&quot;Sign&quot;</span>))</code></pre></div>
<p>Then, I group by the new variable to see how many school have a significative <span class="math inline">\(\alpha\)</span> parameter:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_lin_sum_alphas <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(sign) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(sign) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">n=</span><span class="kw">n</span>())</code></pre></div>
<pre><code>## # A tibble: 2 x 2
##   sign       n
##   &lt;chr&gt;  &lt;int&gt;
## 1 NoSign   123
## 2 Sign      25</code></pre>
<p>Out of 148 primary schools, only 25 schools show to have somewhat an effect on the score.</p>
<p>These are such schools:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_lin_sum_alphas <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(sign <span class="op">==</span><span class="st"> &#39;Sign&#39;</span>)</code></pre></div>
<pre><code>## # A tibble: 25 x 12
##    param    mean se_mean    sd   `2.5%`   `25%`  `50%`   `75%` `97.5%`
##    &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1 alph~   8.77  0.121   6.21    0.290    4.04   7.42   12.2    23.4  
##  2 alph~   1.20  0.00485 0.353   0.528    0.951  1.20    1.44    1.88 
##  3 alph~  -0.992 0.00552 0.446  -1.85    -1.30  -0.993  -0.694  -0.105
##  4 alph~   0.859 0.00541 0.446   0.0198   0.554  0.846   1.16    1.74 
##  5 alph~   1.85  0.00993 0.715   0.542    1.35   1.81    2.30    3.34 
##  6 alph~  -9.76  0.115   5.78  -23.0    -13.2   -8.54   -5.36   -1.93 
##  7 alph~  10.1   0.137   6.00    1.97     5.53   8.77   13.3    24.8  
##  8 alph~   1.12  0.00511 0.382   0.396    0.872  1.12    1.37    1.87 
##  9 alph~   1.29  0.00576 0.452   0.419    0.990  1.29    1.59    2.17 
## 10 alph~   1.11  0.00436 0.337   0.470    0.886  1.11    1.34    1.80 
## # ... with 15 more rows, and 3 more variables: n_eff &lt;dbl&gt;, Rhat &lt;dbl&gt;,
## #   sign &lt;chr&gt;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mcmc_intervals</span>(<span class="kw">as.array</span>(fit_lin_hier_pri_interc), <span class="dt">pars =</span>
                 fit_lin_sum_alphas <span class="op">%&gt;%</span>
<span class="st">                 </span><span class="kw">filter</span>(sign <span class="op">==</span><span class="st"> &#39;Sign&#39;</span>) <span class="op">%&gt;%</span>
<span class="st">                 </span><span class="kw">pull</span>(param))</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p></li>
</ul>
<p>Analysis on some schools:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(primary <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">139</span>, <span class="dv">69</span>, <span class="dv">49</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(primary, attain01) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarise</span>(<span class="dt">n=</span><span class="kw">n</span>())</code></pre></div>
<pre><code>## # A tibble: 6 x 3
## # Groups:   primary [4]
##   primary attain01     n
##   &lt;fct&gt;   &lt;fct&gt;    &lt;int&gt;
## 1 3       1            3
## 2 49      0           14
## 3 49      1           17
## 4 69      0            7
## 5 69      1            1
## 6 139     0           19</code></pre>
<p>The Brier score:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist_fun</span>(data<span class="op">$</span>attain01<span class="op">%&gt;%</span>as.character<span class="op">%&gt;%</span>as.numeric,
         <span class="kw">as.matrix</span>(fit_lin_hier_pri_interc, <span class="dt">pars =</span> <span class="st">&quot;y_rep&quot;</span>),
         DescTools<span class="op">::</span>BrierScore)</code></pre></div>
<pre><code>## [1] &quot;average statistic = 0.254278748180495&quot;
## [1] &quot;sd = 0.00663399163972432&quot;</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-53-1.png" width="672" /></p>
<p>The model improves the Brier Score (ca 0.25 vs 0.28 for the non-hierarchical model): however, we can see that this improvement seems significative:</p>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-54-1.png" width="672" /></p>
<p>This is not though confirmed by the LOOIC:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="dt">loo_pri=</span>loo<span class="op">::</span><span class="kw">loo</span>(loo<span class="op">::</span><span class="kw">extract_log_lik</span>(fit_lin_hier_pri_interc)))</code></pre></div>
<pre><code>## Warning: Relative effective sample sizes (&#39;r_eff&#39; argument) not specified.
## For models fit with MCMC, the reported PSIS effective sample sizes and 
## MCSE estimates will be over-optimistic.</code></pre>
<pre><code>## Warning: Some Pareto k diagnostic values are too high. See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<pre><code>## 
## Computed from 4000 by 3435 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1511.9 36.2
## p_loo       154.9  6.6
## looic      3023.9 72.3
## ------
## Monte Carlo SE of elpd_loo is NA.
## 
## Pareto k diagnostic values:
##                          Count Pct.    Min. n_eff
## (-Inf, 0.5]   (good)     3371  98.1%   812       
##  (0.5, 0.7]   (ok)         44   1.3%   362       
##    (0.7, 1]   (bad)        12   0.3%   52        
##    (1, Inf)   (very bad)    8   0.2%   28        
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">loo_soc</code></pre></div>
<pre><code>## 
## Computed from 4000 by 3435 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1469.8 32.0
## p_loo         3.9  0.2
## looic      2939.6 63.9
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &lt; 0.5).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">loo_<span class="dv">1</span></code></pre></div>
<pre><code>## 
## Computed from 4000 by 3435 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1504.7 31.8
## p_loo         2.0  0.1
## looic      3009.4 63.6
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &lt; 0.5).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<p>Since the LOOIC operates on the predictive level, it’s possible that a model showing increased goodness of fit but decreased LOOIC may be overfitting the data.</p>
</div>
<div id="stronger-priors" class="section level4">
<h4><span class="header-section-number">4.2.2.2</span> Stronger priors</h4>
<p>Fit a new model with stronger priors:</p>
<p><span class="math display">\[
\alpha \sim N(0.2,1) \\
\beta \sim N(0,1)
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_lin_hier_pri_interc_s =<span class="st"> </span><span class="kw">sampling</span>(model_lin_hier_interc_s, 
  <span class="dt">data =</span> <span class="kw">list</span>(<span class="dt">N =</span> data <span class="op">%&gt;%</span><span class="st"> </span>nrow,
               <span class="dt">x =</span> data<span class="op">$</span>verbal,
               <span class="dt">y =</span> y,
               <span class="dt">G =</span> data<span class="op">$</span>primary<span class="op">%&gt;%</span>levels<span class="op">%&gt;%</span>length,
               <span class="dt">group_mapping =</span> data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(primary) <span class="op">%&gt;%</span>
<span class="st">                                </span>as.numeric),
  <span class="dt">cores =</span> <span class="dv">4</span>)

<span class="kw">saveRDS</span>(fit_lin_hier_pri_interc_s, <span class="st">&quot;models/fit_lin_2s.rds&quot;</span>)</code></pre></div>
<ul>
<li><p>Analysis on <span class="math inline">\(\alpha\)</span>s:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_lin_sum =<span class="st"> </span><span class="kw">summary</span>(fit_lin_hier_pri_interc_s)
fit_lin_sum =<span class="st"> </span>fit_lin_sum<span class="op">$</span>summary <span class="op">%&gt;%</span>
<span class="st">  </span>as_tibble <span class="op">%&gt;%</span>
<span class="st">  </span>tibble<span class="op">::</span><span class="kw">add_column</span>(<span class="dt">param =</span> fit_lin_sum<span class="op">$</span>summary <span class="op">%&gt;%</span><span class="st"> </span>row.names) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(param, <span class="kw">everything</span>())</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_lin_sum =<span class="st"> </span>fit_lin_sum <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">sig =</span> <span class="kw">ifelse</span>(<span class="st">`</span><span class="dt">2.5%</span><span class="st">`</span><span class="op">*</span><span class="st">`</span><span class="dt">97.5%</span><span class="st">`</span><span class="op">&lt;=</span><span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>))

<span class="kw">print</span>(fit_lin_sum <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(<span class="kw">grepl</span>(<span class="st">&quot;alpha&quot;</span>,param), sig<span class="op">==</span><span class="dv">0</span>) <span class="op">%&gt;%</span><span class="st"> </span>count)</code></pre></div>
<pre><code>## # A tibble: 1 x 1
##       n
##   &lt;int&gt;
## 1   135</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(fit_lin_sum <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(<span class="kw">grepl</span>(<span class="st">&quot;alpha&quot;</span>,param), sig<span class="op">==</span><span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span>count)</code></pre></div>
<pre><code>## # A tibble: 1 x 1
##       n
##   &lt;int&gt;
## 1    13</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mcmc_intervals</span>(<span class="kw">as.array</span>(fit_lin_hier_pri_interc_s), <span class="dt">pars =</span>
                     fit_lin_sum <span class="op">%&gt;%</span>
<span class="st">                     </span><span class="kw">filter</span>(sig <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, <span class="kw">grepl</span>(<span class="st">&quot;alpha&quot;</span>,param)) <span class="op">%&gt;%</span>
<span class="st">                     </span><span class="kw">pull</span>(param))</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-62-1.png" width="672" /></p></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">loo_pri_s =<span class="st"> </span>loo<span class="op">::</span><span class="kw">loo</span>(loo<span class="op">::</span><span class="kw">extract_log_lik</span>(fit_lin_hier_pri_interc_s))</code></pre></div>
<pre><code>## Warning: Relative effective sample sizes (&#39;r_eff&#39; argument) not specified.
## For models fit with MCMC, the reported PSIS effective sample sizes and 
## MCSE estimates will be over-optimistic.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(loo_pri)</code></pre></div>
<pre><code>## 
## Computed from 4000 by 3435 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1511.9 36.2
## p_loo       154.9  6.6
## looic      3023.9 72.3
## ------
## Monte Carlo SE of elpd_loo is NA.
## 
## Pareto k diagnostic values:
##                          Count Pct.    Min. n_eff
## (-Inf, 0.5]   (good)     3371  98.1%   812       
##  (0.5, 0.7]   (ok)         44   1.3%   362       
##    (0.7, 1]   (bad)        12   0.3%   52        
##    (1, Inf)   (very bad)    8   0.2%   28        
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(loo_<span class="dv">1</span>)</code></pre></div>
<pre><code>## 
## Computed from 4000 by 3435 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1504.7 31.8
## p_loo         2.0  0.1
## looic      3009.4 63.6
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &lt; 0.5).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(loo_soc)</code></pre></div>
<pre><code>## 
## Computed from 4000 by 3435 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1469.8 32.0
## p_loo         3.9  0.2
## looic      2939.6 63.9
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &lt; 0.5).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
</div>
<div id="model-with-variable-intercept-over-primary-schools-grouped-together-in-clusters" class="section level4">
<h4><span class="header-section-number">4.2.2.3</span> Model with variable intercept over primary schools grouped together in clusters</h4>
<p>This model is done merely for showing how, by reducing the number of parameters, we can reduce overfitting in the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data =<span class="st"> </span>data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">school_clu =</span> 
            <span class="kw">ifelse</span>(primary <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">16</span>,<span class="dv">69</span>,<span class="dv">139</span>), <span class="dv">1</span>,
            <span class="kw">ifelse</span>(primary <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">6</span>,<span class="dv">37</span>,<span class="dv">47</span>,<span class="dv">49</span>,<span class="dv">61</span>,<span class="dv">88</span>,<span class="dv">99</span>,<span class="dv">116</span>,<span class="dv">132</span>,<span class="dv">143</span>), <span class="dv">2</span>,
                   <span class="dv">3</span>)))

data<span class="op">$</span>school_clu =<span class="st"> </span>data<span class="op">$</span>school_clu <span class="op">%&gt;%</span><span class="st"> </span>as.factor</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_lin_hier_clu_interc =<span class="st"> </span><span class="kw">sampling</span>(model_lin_hier_interc_s, 
  <span class="dt">data =</span> <span class="kw">list</span>(<span class="dt">N =</span> data <span class="op">%&gt;%</span><span class="st"> </span>nrow,
               <span class="dt">x =</span> data<span class="op">$</span>verbal,
               <span class="dt">y =</span> y,
               <span class="dt">G =</span> data<span class="op">$</span>school_clu<span class="op">%&gt;%</span>levels<span class="op">%&gt;%</span>length,
               <span class="dt">group_mapping =</span> data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(school_clu) <span class="op">%&gt;%</span>
<span class="st">                                </span>as.numeric),
  <span class="dt">cores =</span> <span class="dv">4</span>)

<span class="kw">saveRDS</span>(fit_lin_hier_clu_interc, <span class="st">&quot;models/fit_lin_clu.rds&quot;</span>)</code></pre></div>
<p>Let’s draw the posteriors’ graph like before:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mcmc_intervals</span>(fit_lin_hier_clu_interc <span class="op">%&gt;%</span><span class="st"> </span>as.array, <span class="dt">pars =</span>
                 <span class="kw">dimnames</span>(fit_lin_hier_clu_interc)<span class="op">$</span>parameters <span class="op">%&gt;%</span>
<span class="st">                 </span>as_tibble <span class="op">%&gt;%</span>
<span class="st">                 </span><span class="kw">filter</span>(<span class="kw">grepl</span>(<span class="st">&quot;alpha|beta&quot;</span>,value)) <span class="op">%&gt;%</span><span class="st"> </span>pull)</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-67-1.png" width="672" /></p>
<p>The previously defined clusters behave as expected:</p>
<ul>
<li>the first group has a large positive coefficient with large variance</li>
<li>the second group has a large negative coefficient with large variance</li>
<li>the fifth group has a positive coefficient with mean very close to 0 (0.14), although 0 is not within the 95% CI (which starts at 0.04)</li>
</ul>
<p>Moreover, <span class="math inline">\(\beta\)</span> has a value which is very close to that obtained in all of the previous models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ppc_stat_grouped</span>(y, <span class="dt">yrep =</span> <span class="kw">as.matrix</span>(fit_lin_hier_clu_interc, <span class="dt">pars =</span> <span class="st">&quot;y_rep&quot;</span>), <span class="dt">group =</span> data<span class="op">$</span>school_clu )</code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-68-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist_fun</span>(data<span class="op">$</span>attain01<span class="op">%&gt;%</span>as.character<span class="op">%&gt;%</span>as.numeric,
         <span class="kw">as.matrix</span>(fit_lin_hier_clu_interc, <span class="dt">pars =</span> <span class="st">&quot;y_rep&quot;</span>),
         DescTools<span class="op">::</span>BrierScore)</code></pre></div>
<pre><code>## [1] &quot;average statistic = 0.274668413391558&quot;
## [1] &quot;sd = 0.00709323331014975&quot;</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-69-1.png" width="672" /></p>
<p>This model shows a worse fit than the previous, but that might be attributed to the previous one overfitting the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">loo<span class="op">::</span><span class="kw">compare</span>(
  loo<span class="op">::</span><span class="kw">loo</span>(loo<span class="op">::</span><span class="kw">extract_log_lik</span>(fit_lin_hier_pri_interc)),
  loo<span class="op">::</span><span class="kw">loo</span>(loo<span class="op">::</span><span class="kw">extract_log_lik</span>(fit_lin_hier_clu_interc))
)</code></pre></div>
<pre><code>## Warning: Relative effective sample sizes (&#39;r_eff&#39; argument) not specified.
## For models fit with MCMC, the reported PSIS effective sample sizes and 
## MCSE estimates will be over-optimistic.</code></pre>
<pre><code>## Warning: Some Pareto k diagnostic values are too high. See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<pre><code>## Warning: Relative effective sample sizes (&#39;r_eff&#39; argument) not specified.
## For models fit with MCMC, the reported PSIS effective sample sizes and 
## MCSE estimates will be over-optimistic.</code></pre>
<pre><code>## elpd_diff        se 
##      54.3      15.2</code></pre>
<p>The LOOIC large positive difference tells us that this model with clustered schools has a larger predictive power than the previous one.</p>
</div>
</div>
<div id="hierarchy-over-secondary-school" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Hierarchy over secondary school</h3>
<p>As a final model class, I’ll try to fit a model with hierarchy over the secondary school instead of the primary. My goal is to check whether we can use the primary school provenance or the secondary school belonging to predict better whether the test will be passed or not.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_lin_hier_sec_interc =<span class="st"> </span><span class="kw">sampling</span>(model_lin_hier_interc, 
  <span class="dt">data =</span> <span class="kw">list</span>(<span class="dt">N =</span> data <span class="op">%&gt;%</span><span class="st"> </span>nrow,
               <span class="dt">x =</span> data<span class="op">$</span>verbal,
               <span class="dt">y =</span> y,
               <span class="dt">G =</span> data<span class="op">$</span>second<span class="op">%&gt;%</span>levels<span class="op">%&gt;%</span>length,
               <span class="dt">group_mapping =</span> data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(second) <span class="op">%&gt;%</span>
<span class="st">                                </span>as.numeric),
  <span class="dt">cores =</span> <span class="dv">4</span>)

<span class="kw">saveRDS</span>(fit_lin_hier_sec_interc, <span class="st">&quot;models/fit_lin_sec.rds&quot;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mcmc_intervals</span>(fit_lin_hier_sec_interc <span class="op">%&gt;%</span><span class="st"> </span>as.array, <span class="dt">pars =</span>
                 <span class="kw">dimnames</span>(fit_lin_hier_sec_interc)<span class="op">$</span>parameters <span class="op">%&gt;%</span>
<span class="st">                 </span>as_tibble <span class="op">%&gt;%</span>
<span class="st">                 </span><span class="kw">filter</span>(<span class="kw">grepl</span>(<span class="st">&quot;alpha|beta&quot;</span>,value)) <span class="op">%&gt;%</span><span class="st"> </span>pull)</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-73-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">loo_sec =<span class="st">  </span>loo<span class="op">::</span><span class="kw">loo</span>(loo<span class="op">::</span><span class="kw">extract_log_lik</span>(fit_lin_hier_sec_interc))</code></pre></div>
<pre><code>## Warning: Relative effective sample sizes (&#39;r_eff&#39; argument) not specified.
## For models fit with MCMC, the reported PSIS effective sample sizes and 
## MCSE estimates will be over-optimistic.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(loo_sec)</code></pre></div>
<pre><code>## 
## Computed from 4000 by 3435 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1502.6 32.3
## p_loo        20.5  0.6
## looic      3005.1 64.6
## ------
## Monte Carlo SE of elpd_loo is 0.1.
## 
## All Pareto k estimates are good (k &lt; 0.5).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(loo_<span class="dv">1</span>)</code></pre></div>
<pre><code>## 
## Computed from 4000 by 3435 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1504.7 31.8
## p_loo         2.0  0.1
## looic      3009.4 63.6
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &lt; 0.5).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(loo_soc)</code></pre></div>
<pre><code>## 
## Computed from 4000 by 3435 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1469.8 32.0
## p_loo         3.9  0.2
## looic      2939.6 63.9
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &lt; 0.5).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
</div>
<div id="cross-hierarchy-with-secondary-school-and-social-extraction" class="section level3">
<h3><span class="header-section-number">4.2.4</span> Cross-hierarchy with secondary school and social extraction</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data =<span class="st"> </span>data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">soc_hi =</span> <span class="kw">factor</span>(<span class="kw">ifelse</span>(social_cat<span class="op">==</span><span class="st">&quot;low&quot;</span>,<span class="dv">0</span>,<span class="dv">1</span>)))
data =<span class="st"> </span>data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">soc_sec=</span><span class="kw">factor</span>(tidyr<span class="op">::</span><span class="kw">unite</span>(data,<span class="st">&quot;soc_sec&quot;</span>,<span class="kw">c</span>(<span class="st">&#39;soc_hi&#39;</span>,<span class="st">&#39;second&#39;</span>))<span class="op">$</span>soc_sec))

data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(soc_sec) <span class="op">%&gt;%</span><span class="st"> </span>table <span class="op">%&gt;%</span><span class="st"> </span>as.matrix</code></pre></div>
<pre><code>##      [,1]
## 0_1   175
## 0_10   64
## 0_11  146
## 0_12  151
## 0_13  136
## 0_14  193
## 0_15  104
## 0_16   99
## 0_17  160
## 0_18  211
## 0_19  103
## 0_2   148
## 0_3    98
## 0_4    86
## 0_5   117
## 0_6   174
## 0_7    86
## 0_8    90
## 0_9    91
## 1_1    44
## 1_10   28
## 1_11   88
## 1_12  102
## 1_13   80
## 1_14   97
## 1_15   43
## 1_16   35
## 1_17   73
## 1_18   46
## 1_19    8
## 1_2    51
## 1_3    58
## 1_4    53
## 1_5    58
## 1_6    76
## 1_7    23
## 1_8    17
## 1_9    23</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_lin_hier_soc_sec_interc =<span class="st"> </span><span class="kw">sampling</span>(model_lin_hier_interc_s, 
  <span class="dt">data =</span> <span class="kw">list</span>(<span class="dt">N =</span> data <span class="op">%&gt;%</span><span class="st"> </span>nrow,
               <span class="dt">x =</span> data<span class="op">$</span>verbal,
               <span class="dt">y =</span> y,
               <span class="dt">G =</span> data<span class="op">$</span>soc_sec<span class="op">%&gt;%</span>levels<span class="op">%&gt;%</span>length,
               <span class="dt">group_mapping =</span> data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(soc_sec) <span class="op">%&gt;%</span>
<span class="st">                                </span>as.numeric),
  <span class="dt">cores =</span> <span class="dv">4</span>)

<span class="kw">saveRDS</span>(fit_lin_hier_soc_sec_interc, <span class="st">&quot;models/fit_lin_soc_sec.rds&quot;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mcmc_intervals</span>(fit_lin_hier_soc_sec_interc <span class="op">%&gt;%</span><span class="st"> </span>as.array, <span class="dt">pars =</span>
                 <span class="kw">dimnames</span>(fit_lin_hier_soc_sec_interc)<span class="op">$</span>parameters <span class="op">%&gt;%</span>
<span class="st">                 </span>as_tibble <span class="op">%&gt;%</span>
<span class="st">                 </span><span class="kw">filter</span>(<span class="kw">grepl</span>(<span class="st">&quot;alpha|beta&quot;</span>,value)) <span class="op">%&gt;%</span><span class="st"> </span>pull)</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-78-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">loo_soc_sec =<span class="st"> </span>loo<span class="op">::</span><span class="kw">loo</span>(loo<span class="op">::</span><span class="kw">extract_log_lik</span>(fit_lin_hier_soc_sec_interc))</code></pre></div>
<pre><code>## Warning: Relative effective sample sizes (&#39;r_eff&#39; argument) not specified.
## For models fit with MCMC, the reported PSIS effective sample sizes and 
## MCSE estimates will be over-optimistic.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(loo_soc_sec)</code></pre></div>
<pre><code>## 
## Computed from 4000 by 3435 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1480.4 32.4
## p_loo        35.0  1.1
## looic      2960.9 64.9
## ------
## Monte Carlo SE of elpd_loo is 0.1.
## 
## All Pareto k estimates are good (k &lt; 0.5).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(loo_<span class="dv">1</span>)</code></pre></div>
<pre><code>## 
## Computed from 4000 by 3435 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1504.7 31.8
## p_loo         2.0  0.1
## looic      3009.4 63.6
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &lt; 0.5).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(loo_soc)</code></pre></div>
<pre><code>## 
## Computed from 4000 by 3435 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1469.8 32.0
## p_loo         3.9  0.2
## looic      2939.6 63.9
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &lt; 0.5).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(loo_sec)</code></pre></div>
<pre><code>## 
## Computed from 4000 by 3435 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1502.6 32.3
## p_loo        20.5  0.6
## looic      3005.1 64.6
## ------
## Monte Carlo SE of elpd_loo is 0.1.
## 
## All Pareto k estimates are good (k &lt; 0.5).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="preliminary-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="conclusion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
